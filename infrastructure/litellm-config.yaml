# LiteLLM Proxy Configuration
# Routes requests to multiple Ollama instances across your server garden
#
# Usage:
#   pip install litellm[proxy]
#   litellm --config litellm-config.yaml
#
# Then point Crush/Aider at: http://localhost:4000

model_list:
  # ============================================
  # ubuntu25 (RX 590 8GB) - localhost (LiteLLM runs here)
  # Fast eval rate, dedicated VRAM
  # ============================================
  # TIER 1: Fast/Tiny Models (Search & Routing)
  - model_name: "fast/tinyllama"
    litellm_params:
      model: "ollama/tinyllama"
      api_base: "http://localhost:11434"
    model_info:
      description: "Ultra-fast for search queries and routing decisions"

  - model_name: "fast/llama3.2-3b"
    litellm_params:
      model: "ollama/llama3.2:3b"
      api_base: "http://localhost:11434"
    model_info:
      description: "Fast with tool support"

  # ============================================
  # TIER 2: Medium Models (Code & Reasoning)
  # ============================================
  - model_name: "code/deepseek-6.7b"
    litellm_params:
      model: "ollama/deepseek-coder:6.7b"
      api_base: "http://localhost:11434"
    model_info:
      description: "Code specialist"

  - model_name: "code/mistral-7b"
    litellm_params:
      model: "ollama/mistral:7b"
      api_base: "http://localhost:11434"
    model_info:
      description: "General reasoning"

  - model_name: "tools/groq-8b"
    litellm_params:
      model: "ollama/llama3-groq-tool-use:8b"
      api_base: "http://localhost:11434"
    model_info:
      description: "Tool calling specialist"

  # ============================================
  # MacBook Air (M4 32GB) - 192.168.4.47
  # Fast prompt processing, tool-use models
  # ============================================
  - model_name: "air/qwen-3b"
    litellm_params:
      model: "ollama/qwen2.5-coder:3b"
      api_base: "http://192.168.4.47:11434"
    model_info:
      description: "Fast coding model on MacBook Air"

  - model_name: "air/tinyllama"
    litellm_params:
      model: "ollama/tinyllama"
      api_base: "http://192.168.4.47:11434"
    model_info:
      description: "Ultra-fast on MacBook Air"

  - model_name: "tools/groq-8b-air"
    litellm_params:
      model: "ollama/llama3-groq-tool-use:8b"
      api_base: "http://192.168.4.47:11434"
    model_info:
      description: "Tool calling on MacBook Air (needs pull)"

  # ============================================
  # Mac Mini (M4 16GB) - 192.168.4.41
  # Always-on, larger models
  # ============================================
  - model_name: "mini/qwen-14b"
    litellm_params:
      model: "ollama/qwen2.5-coder:14b"
      api_base: "http://192.168.4.41:11434"
    model_info:
      description: "Large coding model on Mac Mini"

  - model_name: "mini/qwen-7b"
    litellm_params:
      model: "ollama/qwen2.5-coder:7b"
      api_base: "http://192.168.4.41:11434"
    model_info:
      description: "Medium coding model on Mac Mini"

# General settings
litellm_settings:
  drop_params: true  # Ignore unsupported params
  set_verbose: false

# Optional: Add routing rules
# router_settings:
#   routing_strategy: "least-busy"  # or "simple-shuffle", "latency-based"
