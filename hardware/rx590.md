# AMD Radeon RX 590

> *GFX eight-oh-three*
> *AMD forgot you exist*
> *Vulkan sets you free*

## The Struggle

The RX 590 is a Polaris-based GPU (gfx803) that AMD has effectively abandoned for compute workloads. ROCm dropped support after version 5.x, leaving owners with a capable 8GB GPU that can't run official AMD ML stacks.

**The salvation:** Vulkan backend in llama.cpp/Ollama bypasses ROCm entirely, giving this card new life for local LLM inference.

## Specifications

| Spec | Value |
|------|-------|
| **Model** | AMD Radeon RX 590 |
| **Architecture** | Polaris 30 (GCN 4.0) |
| **Compute Units** | 36 |
| **Stream Processors** | 2304 |
| **VRAM** | 8GB GDDR5 |
| **Memory Bus** | 256-bit |
| **Memory Bandwidth** | 256 GB/s |
| **TDP** | 225W |
| **gfx ID** | gfx803 |

## The VRAM Problem

8GB sounds generous until you try to run modern LLMs:

| Model Size | VRAM Usage | Fits? |
|------------|------------|-------|
| 3B Q4 | ~2.5GB | ✅ Comfortable |
| 7B Q4 | ~5-6GB | ✅ Good fit |
| 8B Q4 | ~6.5GB | ⚠️ Tight |
| 13B Q4 | ~8.5GB | ❌ Spills to RAM |
| 14B+ Q4 | ~10GB+ | ❌ Mostly CPU |

**The math is cruel:** Every parameter needs memory, and 8GB is the new minimum for serious LLM work. The RX 590 lives on the edge.

## Backend Evolution

### ROCm (The Broken Promise)
```
ROCm 5.x: Last version with gfx803 support
ROCm 6.x: "What's a Polaris?"
```

AMD dropped gfx803 from ROCm, breaking PyTorch, TensorFlow, and any official ML stack.

### Vulkan (The Savior)

llama.cpp added Vulkan support, bypassing ROCm entirely:

```bash
# The magic environment variables
export OLLAMA_VULKAN=true
export GGML_VK_VISIBLE_DEVICES=0  # Critical: disable Intel iGPU
export HIP_VISIBLE_DEVICES=       # Empty = ignore ROCm
```

**Result:** Full GPU acceleration at ~80-90% of theoretical ROCm performance, with none of the driver headaches.

## Current Performance (ubuntu25)

With Vulkan backend and optimized settings:

| Model | Eval Rate | Prompt Rate |
|-------|-----------|-------------|
| TinyLlama 1.1B | ~150 tok/s | ~200 tok/s |
| Qwen 2.5 3B | ~64 tok/s | ~222 tok/s |
| Qwen 2.5 7B | ~32 tok/s | ~105 tok/s |
| Llama 3.1 8B | ~30 tok/s | ~100 tok/s |

## Ollama Configuration

`/etc/systemd/system/ollama.service.d/override.conf`:

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_VULKAN=true"
Environment="HIP_VISIBLE_DEVICES="
Environment="OLLAMA_FLASH_ATTENTION=1"
Environment="OLLAMA_KV_CACHE_TYPE=q8_0"
Environment="GGML_VK_VISIBLE_DEVICES=0"
```

**Critical:** `GGML_VK_VISIBLE_DEVICES=0` prevents Ollama from trying to split across the Intel iGPU (UHD 630), which causes massive slowdowns.

## Monitoring

```bash
# GPU utilization
watch -n1 "cat /sys/class/drm/card1/device/gpu_busy_percent"

# VRAM usage
watch -n1 "cat /sys/class/drm/card1/device/mem_info_vram_used"

# Temperature
watch -n1 "cat /sys/class/drm/card1/device/hwmon/hwmon*/temp1_input"

# Full stats (if radeontop installed)
radeontop -c
```

## The Verdict

The RX 590 is a **zombie GPU** for ML work—officially dead, but still walking thanks to Vulkan. It won't win any benchmarks against modern NVIDIA cards or Apple Silicon, but for a card you might have lying around, it delivers surprisingly usable local LLM performance.

**Best use case:** Secondary inference server, home lab experiments, or budget-conscious local AI setup.

## References

- [Ollama Vulkan Support](https://github.com/ollama/ollama/blob/main/docs/gpu.md)
- [llama.cpp Vulkan Backend](https://github.com/ggerganov/llama.cpp/blob/master/docs/backend/VULKAN.md)
- [AMD gfx803 Deprecation](https://github.com/ROCm/ROCm/issues/1659)
